{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gis2010/AgentGPT/blob/main/emotion_GPT2_as_text_generator_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy0zjL_2ouOU",
        "outputId": "e12e16c1-7815-411f-db02-e40a3bd05659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/18: 100%|██████████| 1125/1125 [00:53<00:00, 20.98it/s, Loss=0.613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average loss: 0.6127, Test accuracy: 0.7605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.62it/s, Loss=0.353]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average loss: 0.3532, Test accuracy: 0.7970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.61it/s, Loss=0.237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average loss: 0.2375, Test accuracy: 0.8530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/18: 100%|██████████| 1125/1125 [00:55<00:00, 20.45it/s, Loss=0.184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Average loss: 0.1843, Test accuracy: 0.8985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.68it/s, Loss=0.146]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Average loss: 0.1457, Test accuracy: 0.9175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.63it/s, Loss=0.121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Average loss: 0.1208, Test accuracy: 0.9215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.50it/s, Loss=0.103]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Average loss: 0.1028, Test accuracy: 0.9260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.62it/s, Loss=0.0927]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Average loss: 0.0927, Test accuracy: 0.9260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.59it/s, Loss=0.0887]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Average loss: 0.0887, Test accuracy: 0.9330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.58it/s, Loss=0.079]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Average loss: 0.0790, Test accuracy: 0.9315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.69it/s, Loss=0.0771]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - Average loss: 0.0771, Test accuracy: 0.9325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.60it/s, Loss=0.0699]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 - Average loss: 0.0699, Test accuracy: 0.9345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.60it/s, Loss=0.0663]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 - Average loss: 0.0663, Test accuracy: 0.9265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.61it/s, Loss=0.064]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 - Average loss: 0.0640, Test accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.60it/s, Loss=0.0633]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 - Average loss: 0.0633, Test accuracy: 0.9380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/18: 100%|██████████| 1125/1125 [00:54<00:00, 20.75it/s, Loss=0.0605]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 - Average loss: 0.0605, Test accuracy: 0.9390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/18: 100%|██████████| 1125/1125 [00:53<00:00, 20.85it/s, Loss=0.0571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 - Average loss: 0.0571, Test accuracy: 0.9370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/18: 100%|██████████| 1125/1125 [00:55<00:00, 20.26it/s, Loss=0.0574]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 - Average loss: 0.0574, Test accuracy: 0.9420\n",
            "Training accuracy: 0.9424\n",
            "Test accuracy: 0.9420\n",
            "Using device: cuda\n",
            "Input: I'm so happy to be able to finetune an LLM!\n",
            "Generated emotion: joy\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import random\n",
        "import gzip\n",
        "import requests\n",
        "import torch\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Build prompt for emotion classification\n",
        "def build_prompt(text):\n",
        "    return f\"Predict the emotion for the following text: {text}\\nEmotion:\"\n",
        "\n",
        "# Encode text using tokenizer\n",
        "def encode_text(tokenizer, text, return_tensor=False):\n",
        "    if return_tensor:\n",
        "        return tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    else:\n",
        "        return tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "# Decode token IDs back to text\n",
        "def decode_text(tokenizer, token_ids):\n",
        "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "# Define HDC operations\n",
        "class HDCOperations:\n",
        "    def __init__(self, dim=10000):\n",
        "        self.dim = dim\n",
        "\n",
        "    def bind(self, a, b):\n",
        "        \"\"\"\n",
        "        Binding operation: XOR for binary vectors.\n",
        "        \"\"\"\n",
        "        return a ^ b\n",
        "\n",
        "    def unbind(self, a, b):\n",
        "        \"\"\"\n",
        "        Unbinding operation: XOR for binary vectors (self-inverse).\n",
        "        \"\"\"\n",
        "        return a ^ b\n",
        "\n",
        "    def bundle(self, vectors):\n",
        "        \"\"\"\n",
        "        Bundling operation: Thresholded sum for binary vectors.\n",
        "        \"\"\"\n",
        "        return (sum(vectors) > len(vectors) / 2).int()\n",
        "\n",
        "# HDC-based memory storage\n",
        "class HDCMemory:\n",
        "    def __init__(self, dim=10000):\n",
        "        self.dim = dim\n",
        "        self.memory = {}\n",
        "        self.hdc_ops = HDCOperations(dim)\n",
        "\n",
        "    def store(self, key, value):\n",
        "        \"\"\"\n",
        "        Store a key-value pair using HDC binding.\n",
        "        \"\"\"\n",
        "        self.memory[key] = self.hdc_ops.bind(key, value)\n",
        "\n",
        "    def retrieve(self, key):\n",
        "        \"\"\"\n",
        "        Retrieve a value using HDC unbinding.\n",
        "        \"\"\"\n",
        "        if key in self.memory:\n",
        "            return self.hdc_ops.unbind(key, self.memory[key])\n",
        "        else:\n",
        "            raise KeyError(\"Key not found in memory.\")\n",
        "\n",
        "    def bundle(self, keys):\n",
        "        \"\"\"\n",
        "        Bundle multiple keys into a single vector.\n",
        "        \"\"\"\n",
        "        vectors = [self.memory[key] for key in keys]\n",
        "        return self.hdc_ops.bundle(vectors)\n",
        "\n",
        "# Dataset class with HDC integration\n",
        "class PromptCompletionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, hdc_memory):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.hdc_memory = hdc_memory\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        prompt = item[\"prompt\"]\n",
        "        completion = item[\"completion\"]\n",
        "\n",
        "        # Encode prompt and completion as high-dimensional vectors\n",
        "        encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
        "        encoded_completion = self.tokenizer.encode(completion, add_special_tokens=False)\n",
        "\n",
        "        # Store in HDC memory\n",
        "        key = torch.tensor(encoded_prompt, dtype=torch.int)\n",
        "        value = torch.tensor(encoded_completion, dtype=torch.int)\n",
        "        self.hdc_memory.store(key, value)\n",
        "\n",
        "        # Retrieve from HDC memory (for demonstration)\n",
        "        retrieved_value = self.hdc_memory.retrieve(key)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoded_prompt + encoded_completion + [self.tokenizer.eos_token_id],\n",
        "            \"labels\": [-100] * len(encoded_prompt) + encoded_completion + [self.tokenizer.eos_token_id],\n",
        "            \"prompt\": prompt,\n",
        "            \"expected_completion\": completion,\n",
        "            \"retrieved_completion\": self.tokenizer.decode(retrieved_value.tolist(), skip_special_tokens=True)\n",
        "        }\n",
        "\n",
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
        "    input_ids = [item[\"input_ids\"] + [tokenizer.pad_token_id] * (max_length - len(item[\"input_ids\"])) for item in batch]\n",
        "    labels = [item[\"labels\"] + [-100] * (max_length - len(item[\"labels\"])) for item in batch]\n",
        "    attention_mask = [[1] * len(item[\"input_ids\"]) + [0] * (max_length - len(item[\"input_ids\"])) for item in batch]\n",
        "    prompts = [item[\"prompt\"] for item in batch]\n",
        "    expected_completions = [item[\"expected_completion\"] for item in batch]\n",
        "    retrieved_completions = [item[\"retrieved_completion\"] for item in batch]\n",
        "\n",
        "    return (\n",
        "        torch.tensor(input_ids),\n",
        "        torch.tensor(attention_mask),\n",
        "        torch.tensor(labels),\n",
        "        prompts,\n",
        "        expected_completions,\n",
        "        retrieved_completions\n",
        "    )\n",
        "\n",
        "# Download and prepare dataset\n",
        "def download_and_prepare_data(data_url, tokenizer, batch_size, test_ratio=0.1):\n",
        "    response = requests.get(data_url)\n",
        "    content = gzip.decompress(response.content).decode()\n",
        "    dataset = [{\"prompt\": build_prompt(entry['text']), \"completion\": entry[\"label\"].strip()} for entry in map(json.loads, content.splitlines())]\n",
        "    random.shuffle(dataset)\n",
        "    split_index = int(len(dataset) * (1 - test_ratio))\n",
        "    train_data = dataset[:split_index]\n",
        "    test_data = dataset[split_index:]\n",
        "\n",
        "    # Initialize HDC memory\n",
        "    hdc_memory = HDCMemory()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PromptCompletionDataset(train_data, tokenizer, hdc_memory)\n",
        "    test_dataset = PromptCompletionDataset(test_data, tokenizer, hdc_memory)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, test_loader, hdc_memory\n",
        "\n",
        "# Calculate accuracy\n",
        "def calculate_accuracy(model, tokenizer, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels, prompts, expected_completions, retrieved_completions in loader:\n",
        "            for prompt, expected_completion in zip(prompts, expected_completions):\n",
        "                generated_text = generate_text(model, tokenizer, prompt)\n",
        "                if generated_text.strip().lower() == expected_completion.strip().lower():\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    model.train()\n",
        "    return accuracy\n",
        "\n",
        "# Generate text using the model\n",
        "def generate_text(model, tokenizer, prompt, max_new_tokens=50):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        attention_mask=input_ids[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )[0]\n",
        "    generated_text = decode_text(tokenizer, output_ids[input_ids[\"input_ids\"].shape[1]:])\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Test the model\n",
        "def test_model(model_path, test_input, hdc_memory):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load saved model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # Configure padding token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Generate and display prediction\n",
        "    prompt = build_prompt(test_input)\n",
        "    generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"Generated emotion: {generated_text}\")\n",
        "\n",
        "    # Retrieve from HDC memory (for demonstration)\n",
        "    key = torch.tensor(tokenizer.encode(prompt, add_special_tokens=False), dtype=torch.int)\n",
        "    retrieved_value = hdc_memory.retrieve(key)\n",
        "    print(f\"Retrieved completion from HDC memory: {tokenizer.decode(retrieved_value.tolist(), skip_special_tokens=True)}\")\n",
        "\n",
        "# Main training script\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(42)\n",
        "\n",
        "    # Configure basic training parameters\n",
        "    data_url = \"https://www.thelmbook.com/data/emotions\"\n",
        "    model_name = \"openai-community/gpt2\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Configure LoRA parameters\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=16,\n",
        "        lora_alpha=32\n",
        "    )\n",
        "\n",
        "    # Load model and apply LoRA configuration\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Get hyperparameters and prepare data\n",
        "    num_epochs, batch_size, learning_rate = 18, 16, 5e-5\n",
        "    train_loader, test_loader, hdc_memory = download_and_prepare_data(data_url, tokenizer, batch_size)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for input_ids, attention_mask, labels, _, _, _ in progress_bar:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update metrics\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            progress_bar.set_postfix({\"Loss\": total_loss / num_batches})\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_acc = calculate_accuracy(model, tokenizer, test_loader)\n",
        "        print(f\"Epoch {epoch+1} - Average loss: {total_loss / num_batches:.4f}, Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(\"./finetuned_model\")\n",
        "    tokenizer.save_pretrained(\"./finetuned_model\")\n",
        "\n",
        "    # Test the finetuned model\n",
        "    test_input = \"I'm so happy to be able to finetune an LLM!\"\n",
        "    test_model(\"./finetuned_model\", test_input, hdc_memory)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}